# Fraud Inference API

A production-ready FastAPI service for serving MLflow models for fraud detection. This service is designed for high-availability, observability, and consistency between training and serving environments.

## Features

- **MLflow Integration**: Loads models directly from an MLflow Model Registry.
- **Dynamic Model Selection**: Automatically selects the best model version based on aliases (`champion`) or stages (`Production` > `Staging`).
- **Dependency Validation**: Fails at startup if the serving environment's dependencies (`scikit-learn`, `xgboost`, etc.) are incompatible with the model's training dependencies.
- **High-Performance API**: Built with FastAPI for asynchronous, high-throughput inference.
- **Observability**:
  - Structured JSON logging with `structlog`.
  - Prometheus metrics for requests, latency, model loads, and more.
  - Request ID tracing via `X-Request-ID` header.
- **Atomic Model Refresh**: Hot-swap models via a protected `/admin/refresh-models` endpoint without downtime.
- **Containerized**: Ships with a multi-stage `Dockerfile` and `docker-compose` files for development and deployment.

## Getting Started

### Local Development with Poetry

1.  **Prerequisites**:
    - Python 3.11+
    - Poetry

2.  **Installation**:
    ```bash
    # Install dependencies
    poetry install
    ```

3.  **Environment Variables**:
    Create a `.env` file in the project root. You can copy the development template:
    ```bash
    cp .env.dev .env
    ```
    Edit the `.env` file to set your `MLFLOW_TRACKING_URI` and other necessary variables.

4.  **Run the Service**:
    ```bash
    poetry run uvicorn app.main:app --reload
    ```
    The API will be available at `http://localhost:8000`.

### Local Development with Docker

1.  **Prerequisites**:
    - Docker
    - Docker Compose

2.  **Build and Run**:
    ```bash
    docker-compose -f docker-compose.dev.yml up --build
    ```
    The API will be available at `http://localhost:8000`. The service will hot-reload when you make changes to the `app/` directory.

## API Documentation

API documentation is automatically generated by FastAPI and is available at:
- **Swagger UI**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`

### Generating OpenAPI Specification

The `openapi.json` file in the root of this repository contains the OpenAPI 3.1.0 specification for this API. It can be regenerated as follows:

1.  Run the service locally:
    ```bash
    poetry run uvicorn app.main:app --host 0.0.0.0 --port 8000
    ```

2.  In a separate terminal, run:
    ```bash
    curl http://127.0.0.1:8000/openapi.json > openapi.json
    ```

This will overwrite the existing `openapi.json` with the latest version generated from the running application.

### Endpoints

- `POST /v1/predict`: Performs inference.
- `GET /v1/version`: Returns service and loaded model versions.
- `GET /api/v1/health/live`: Liveness probe.
- `GET /api/v1/health/ready`: Readiness probe (returns 200 OK only when models are loaded).
- Legacy endpoints `/livez` and `/readyz` redirect to the new health paths.
- `POST /admin/refresh-models`: (Protected) Triggers a hot-swap of all models.
- `GET /metrics`: Exposes Prometheus metrics.

#### Example: Prediction Request (Batch)

```bash
curl -X POST "http://localhost:8000/v1/predict" \
  -H "accept: application/json" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "xgboost",
    "instances": [
      {
        "transaction_id": 1234567890,
        "amount": 150.75,
        "device_type": "mobile",
        "merchant_type": "grocery"
      },
      {
        "transaction_id": 9876543210,
        "amount": 89.99,
        "device_type": "desktop",
        "merchant_type": "electronics"
      }
    ]
  }'

```

#### Example: Admin Refresh Request

```bash
curl -X 'POST' \
  'http://localhost:8000/v1/admin/refresh-models' \
  -H 'accept: application/json'
```

## Configuration

The service is configured via environment variables. See `app/core/config.py` for a full list of available variables. Key variables include:

- `MLFLOW_TRACKING_URI`: **Required**. The URI of your MLflow tracking server.
- `LOG_LEVEL`: `debug`, `info`, `warning`, `error`.
- `WORKERS`: Number of uvicorn worker processes.
- `CORS_ORIGINS`: Comma-separated list of allowed CORS origins.

## Deployment

This service is designed to be deployed as a containerized application, for example, on a Docker Swarm cluster.

A sample `deploy/stack.yml` is provided for deployment with Docker Swarm and Traefik as a reverse proxy.

1.  **Build and Push Image**:
    Build the Docker image and push it to a container registry (e.g., GHCR, Docker Hub).
    ```bash
    docker build -t your-registry/fraud-inference-api:latest .
    docker push your-registry/fraud-inference-api:latest
    ```

2.  **Prepare Environment**:
    - Ensure your Docker Swarm is running and connected.
    - Configure secrets for sensitive environment variables like cloud credentials.
    - Update `deploy/stack.yml` with your actual image name, domain names, and secret names.

3.  **Deploy Stack**:
    ```bash
    docker stack deploy -c deploy/stack.yml fraud-api-stack
    ```